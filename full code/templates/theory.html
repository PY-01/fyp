<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Signika+Negative:wght@300..700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="logo">Machine Learning Made Visual (MLMV)</div>
        
        <!-- Hamburger Icon for Small Screens -->
        <div class="menu-toggle" id="mobile-menu">
            &#9776; <!-- Hamburger icon (three bars) -->
        </div>
        
        <ul class="nav-links" id="nav-links">
            <li><a href="/">Home</a></li>
            
            <li class="dropdown">
                <a href="#" class="dropbtn">Algorithms</a>
                <div class="dropdown-content">
                    <a href="/knn">k-NN</a>
                    <a href="/dc">Decision Trees</a>
                    <a href="/kmeans">k-Means</a>
                </div>
            </li>
            
            <li><a href="/explore">Pre-Built Examples</a></li>
            <li><a href="/theory">Theory</a></li>
            <li><a href="/guides">Guides</a></li>
            <li><a href="/about">About</a></li>
        </ul>
    </nav>

    <!-- Theory Section -->
    <section class="theory-section">
        <h1>Theoretical Background of Machine Learning Algorithms</h1>
        <p>
            Machine Learning (ML) is a subset of artificial intelligence where algorithms are used to enable machines to learn from data and improve their performance without being explicitly programmed. 
            This section provides insights into the core principles and mathematical concepts behind the algorithms featured in MLMV.
        </p>
    
        <h2><i>k</i>-Nearest Neighbors (<i>k</i>-NN)</h2>
        <p>
            <i>k</i>-NN is a simple, yet powerful, algorithm used for <b>classification</b> and <b>regression</b> tasks. It makes predictions by comparing new data points to the '<i>k</i>' nearest points in the dataset. The class most frequent among the neighbors becomes the predicted class.
        </p>
        <p>
            <i>k</i>-NN is a <b>non-parametric algorithm</b>, which means it does not make any assumption on underlying data. It is also called a <b>lazy learner algorithm</b> because it does not learn from the training set immediately; instead, it stores the dataset and at the time of classification, it performs an action on the dataset.
        </p>
        
        <h3>Example: Classifying Cats and Dogs</h3>
        <p>In this example, we use <i>k</i>-NN to classify whether an unknown animal is a cat or a dog based on two features: <b>fur length</b> and <b>weight</b>.</p>
        <ul>
            <li><b>Cat icons</b> represent known cats.</li>
            <li><b>Dog icons</b> represent known dogs.</li>
            <li><b>Question mark icon</b> is the unknown animal that we want to classify.</li>
        </ul>
        
        <!-- Insert the image here -->
        <img src="{{ url_for('static', filename='img/knn_intro.jpg') }}" alt="k-NN Classifying Example">
        
        <h3>How does <i>k</i>-NN work?</h3>
        <ul>
            <li>Step 1: Selecting the optimal value of <i>k</i> which <i>k</i> represent the number of nearest neighbours that needs to be consider while making prediction.</li>
            <li>Step 2: Calculating distance to measure the similarity between target and training data points.</li>
            <li>Step 3: Finding nearest neighbours which the <i>k</i> data points with the smallest distances to the target point are the nearest neighbors</li>
            <li>Step 4: Perform majority voting where the class with the most occurrences among the neighbors becomes the predicted class for the target data point.</li>
        </ul>
        
        <h3>4 types of computing <i>k</i>-NN distance metrics</h3>
        <p>The key to the <i>k</i>-NN algorithm is determining the distance between the query point and the other data points. Determining distance metrics enables decision boundaries. These boundaries create different data point regions. There are different methods used to calculate distance:</p>
        <ul>
            <li>Euclidean distance is the most common distance measure, which measures a straight line between the query point and the other point being measured.</li>
            
            <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                    <mi>d</mi>
                    <mo>(</mo>
                    <mi>p</mi>
                    <mo>,</mo>
                    <mi>q</mi>
                    <mo>)</mo>
                    <mo>=</mo>
                    <msqrt>
                        <mrow>
                            <mo>(</mo>
                            <msub><mi>x</mi><mn>2</mn></msub>
                            <mo>-</mo>
                            <msub><mi>x</mi><mn>1</mn></msub>
                            <mo>)</mo>
                            <msup><mo>+</mo><mo>(</mo></msup>
                            <msub><mi>y</mi><mn>2</mn></msub>
                            <mo>-</mo>
                            <msub><mi>y</mi><mn>1</mn></msub>
                            <mo>)</mo>
                            <msup><mn>2</mn></msup>
                        </mrow>
                    </msqrt>
                </mrow>
            </math>
            
            <ul>
                <li>
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                            <mi>d</mi>
                            <mo>(</mo>
                            <mi>p</mi>
                            <mo>,</mo>
                            <mi>q</mi>
                            <mo>)</mo>
                        </mrow>
                    </math>
                    represents the Euclidean Distance between the two points p and q.
                </li>
                
                <li>
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                            <mo>(</mo>
                            <msub><mi>x</mi><mn>2</mn></msub>
                            <mo>-</mo>
                            <msub><mi>x</mi><mn>1</mn></msub>
                            <mo>)</mo> 
                        </mrow>
                    </math>
                    represents the difference between the x-coordinates of the two points.
                </li>
                
                <li>
                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                            <mo>(</mo>
                            <msub><mi>y</mi><mn>2</mn></msub>
                            <mo>-</mo>
                            <msub><mi>y</mi><mn>1</mn></msub>
                            <mo>)</mo>
                        </mrow>
                    </math>
                    represents the difference between the y-coordinates of the two points.
                </li>
            </ul>
    
            <li></li>
            <li></li>
            <li></li>
        </ul>
    
        <h2>Decision Trees</h2>
        <p>
            Decision Trees are non-parametric supervised learning methods used for classification and regression. A tree-like structure is created where each node represents a feature, each branch a decision rule, and each leaf a class or value.
        </p>
    
        <h2><i>k</i>-Means Clustering</h2>
        <p>
            <i>k</i>-Means is an unsupervised learning algorithm used for clustering tasks. It partitions a dataset into 'k' distinct, non-overlapping clusters by minimizing the variance within each cluster. This is achieved through an iterative process of assigning points to the nearest cluster centroid and recalculating the centroids.
        </p>
    </section>


    <!-- Footer -->
    <footer class="footer">
        <p>Â© 2024 Loo Pei Yin. All rights reserved.</p>
    </footer>

    <script>
        const menuToggle = document.getElementById('mobile-menu');
        const navLinks = document.getElementById('nav-links');
        menuToggle.addEventListener('click', () => {
            navLinks.classList.toggle('show');
        });
    </script>
</body>
</html>
